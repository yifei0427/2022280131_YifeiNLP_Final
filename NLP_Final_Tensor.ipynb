{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP5Bp5o06Vo0",
        "outputId": "c01a6828-3bfc-4aeb-ba62-bf3184905457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.18.1 in /usr/local/lib/python3.12/dist-packages (2.18.1)\n",
            "Requirement already satisfied: transformers==4.38.1 in /usr/local/lib/python3.12/dist-packages (4.38.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (1.76.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.18.1) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.1) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.1) (0.45.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.1) (1.2.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.18.1) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.18.1) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.18.1) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.1) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.1) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.1) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.1) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.1) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.1) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.1) (0.1.2)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: textattack in /usr/local/lib/python3.12/dist-packages (0.3.10)\n",
            "Requirement already satisfied: bert-score>=0.3.5 in /usr/local/lib/python3.12/dist-packages (from textattack) (0.3.13)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (from textattack) (0.8.1)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.12/dist-packages (from textattack) (0.15.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from textattack) (3.20.0)\n",
            "Requirement already satisfied: language-tool-python in /usr/local/lib/python3.12/dist-packages (from textattack) (3.0.0)\n",
            "Requirement already satisfied: lemminflect in /usr/local/lib/python3.12/dist-packages (from textattack) (0.2.3)\n",
            "Requirement already satisfied: lru-dict in /usr/local/lib/python3.12/dist-packages (from textattack) (1.4.1)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from textattack) (4.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textattack) (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from textattack) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from textattack) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from textattack) (1.16.3)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from textattack) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from textattack) (4.38.1)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.12/dist-packages (from textattack) (3.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from textattack) (4.67.1)\n",
            "Requirement already satisfied: word2number in /usr/local/lib/python3.12/dist-packages (from textattack) (1.1)\n",
            "Requirement already satisfied: num2words in /usr/local/lib/python3.12/dist-packages (from textattack) (0.5.14)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from textattack) (10.8.0)\n",
            "Requirement already satisfied: pinyin>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from textattack) (0.4.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: OpenHowNet in /usr/local/lib/python3.12/dist-packages (from textattack) (2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score>=0.3.5->textattack) (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score>=0.3.5->textattack) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score>=0.3.5->textattack) (25.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.4.0->textattack) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->textattack) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->textattack) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->textattack) (2025.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->textattack) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->textattack) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->textattack) (0.6.2)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (1.41.1)\n",
            "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (1.3.1)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (6.3.1)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (5.2.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (5.4.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (0.5.12)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (3.1)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (1.6.1)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (0.9.0)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (0.4.2)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (0.8.1)\n",
            "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from flair->textattack) (2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from language-tool-python->textattack) (5.9.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from language-tool-python->textattack) (0.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textattack) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textattack) (1.5.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Requirement already satisfied: anytree in /usr/local/lib/python3.12/dist-packages (from OpenHowNet->textattack) (2.13.0)\n",
            "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (4.0.0)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.12/dist-packages (from bioc<3.0.0,>=2.0.0->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: botocore<1.42.0,>=1.41.1 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.20.27->flair->textattack) (1.41.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.20.27->flair->textattack) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.16.0,>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from boto3>=1.20.27->flair->textattack) (0.15.0)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from deprecated>=1.2.13->flair->textattack) (2.0.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (3.13.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.4.0->flair->textattack) (4.13.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.4.0->textattack) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect>=1.0.9->flair->textattack) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.2.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score>=0.3.5->textattack) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score>=0.3.5->textattack) (2025.10.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair->textattack) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair->textattack) (5.29.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.4.0->textattack) (1.22.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair->textattack) (1.11.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.8)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair->textattack) (2.4.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.4.0->flair->textattack) (1.7.1)\n",
            "TensorFlow Version: 2.18.1\n"
          ]
        }
      ],
      "source": [
        "# 0.1 切换运行时为 GPU/TPU (Runtime -> Change runtime type)\n",
        "\n",
        "# 0.2 安装必要的库（使用可用版本：tensorflow 2.18.1， transformers 4.38.1）\n",
        "!pip install tensorflow==2.18.1 transformers==4.38.1 scikit-learn nltk\n",
        "!pip install jieba # 专门用于中文分词\n",
        "!pip install textattack\n",
        "\n",
        "# 0.3 引入库并设置环境\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score\n",
        "# 关键变更：使用 TFAutoModelForSequenceClassification 提高兼容性\n",
        "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
        "import jieba\n",
        "import os\n",
        "import re\n",
        "\n",
        "# 设置随机种子\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1.1 Colab环境配置: 文件上传与路径设置 ---\n",
        "# 指定数据路径为 Colab 工作区\n",
        "DATA_PATH = '/content/sample_data/'\n",
        "\n",
        "# 定义实际的输入和标签列名\n",
        "CONTENT_COL = 'specific_dialogue_content'\n",
        "LABEL_COL = 'is_fraud'\n",
        "\n",
        "# --- 1.2 数据集加载与分配 ---\n",
        "try:\n",
        "    train_df = pd.read_csv(DATA_PATH + 'train_results.csv', encoding='utf-8')\n",
        "    test_df = pd.read_csv(DATA_PATH + 'test_results.csv', encoding='utf-8')\n",
        "except FileNotFoundError:\n",
        "    print(\"错误：无法找到数据集文件。请检查文件是否已上传到 '/content/sample_data/' 目录。\")\n",
        "    raise\n",
        "\n",
        "# 清理列名（防止隐藏空格或BOM字符）\n",
        "def clean_columns(df):\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df.columns = df.columns.str.replace('\\ufeff', '')\n",
        "    return df\n",
        "\n",
        "train_df = clean_columns(train_df)\n",
        "test_df = clean_columns(test_df)\n",
        "\n",
        "# 最终检查\n",
        "if CONTENT_COL not in train_df.columns or LABEL_COL not in train_df.columns:\n",
        "    print(\"\\n致命错误：无法在数据集中找到 'specific_dialogue_content' 或 'is_fraud' 列。请检查您的CSV文件头！\")\n",
        "    print(f\"当前列名列表: {train_df.columns.tolist()}\")\n",
        "    raise KeyError(f\"Required columns {CONTENT_COL} or {LABEL_COL} not found.\")\n",
        "\n",
        "# ⬇️⬇️⬇️ 关键修复：标签映射与缺失值处理 ⬇️⬇️⬇️\n",
        "try:\n",
        "    # 1. 转换为字符串并统一转为大写\n",
        "    label_mapping = {'TRUE': 1, 'FALSE': 0}\n",
        "\n",
        "    # 应用映射。注意：NaN值映射后仍为NaN（属于 float 类型）\n",
        "    train_df[LABEL_COL] = train_df[LABEL_COL].astype(str).str.upper().map(label_mapping)\n",
        "    test_df[LABEL_COL] = test_df[LABEL_COL].astype(str).str.upper().map(label_mapping)\n",
        "\n",
        "    # 2. 删除标签列中带有 NaN 的行\n",
        "    initial_train_size = len(train_df)\n",
        "    initial_test_size = len(test_df)\n",
        "\n",
        "    train_df.dropna(subset=[LABEL_COL], inplace=True)\n",
        "    test_df.dropna(subset=[LABEL_COL], inplace=True)\n",
        "\n",
        "    # 3. 转换为整数类型（此时已无 NaN，可安全转换）\n",
        "    y_train = train_df[LABEL_COL].astype(int)\n",
        "    y_test = test_df[LABEL_COL].astype(int)\n",
        "\n",
        "    # 打印处理结果\n",
        "    print(f\"训练集：初始行数 {initial_train_size}，缺失标签删除后剩余 {len(train_df)} 行。\")\n",
        "    print(f\"测试集：初始行数 {initial_test_size}，缺失标签删除后剩余 {len(test_df)} 行。\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n致命错误：标签转换失败。错误信息: {e}\")\n",
        "    raise\n",
        "\n",
        "# 提取数据\n",
        "X_train_raw = train_df[CONTENT_COL].astype(str)\n",
        "X_test_raw = test_df[CONTENT_COL].astype(str)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"训练集大小: {len(X_train_raw)}, 测试集大小: {len(X_test_raw)}\")\n",
        "print(f\"输入内容列: {CONTENT_COL}, 标签列: {LABEL_COL}\")\n",
        "\n",
        "# --- 1.3 传统模型(BiLSTM)的中文分词与向量化 ---\n",
        "def chinese_segment(texts):\n",
        "    return [' '.join(jieba.cut(text)) for text in texts]\n",
        "\n",
        "X_train_seg = chinese_segment(X_train_raw)\n",
        "X_test_seg = chinese_segment(X_test_raw)\n",
        "\n",
        "MAX_WORDS = 20000\n",
        "MAX_LEN = 80\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<unk>\")\n",
        "tokenizer.fit_on_texts(X_train_seg)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_seg)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_seg)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "# --- 1.4 大模型(TFBert)的中文Tokenizer ---\n",
        "BERT_MODEL_NAME = 'bert-base-chinese'\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "\n",
        "def encode_data(texts, tokenizer, max_len=MAX_LEN):\n",
        "    return tokenizer(\n",
        "        list(texts),\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='tf'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHHwCt0x7eVz",
        "outputId": "f8397e4a-6897-4f95-8bbf-2a3f64873538"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练集：初始行数 14363，缺失标签删除后剩余 13635 行。\n",
            "测试集：初始行数 2677，缺失标签删除后剩余 2548 行。\n",
            "训练集大小: 13635, 测试集大小: 2548\n",
            "输入内容列: specific_dialogue_content, 标签列: is_fraud\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model cost 2.205 seconds.\n",
            "DEBUG:jieba:Loading model cost 2.205 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.1 BiLSTM 模型 ---\n",
        "def build_bilstm_model(max_words, max_len, embedding_dim):\n",
        "    input_layer = Input(shape=(max_len,))\n",
        "    x = Embedding(max_words, embedding_dim, input_length=max_len)(input_layer)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=False))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "bilstm_model = build_bilstm_model(MAX_WORDS, MAX_LEN, EMBEDDING_DIM)\n",
        "\n",
        "print(\"\\n--- 训练 BiLSTM ---\")\n",
        "bilstm_model.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "_, bilstm_acc = bilstm_model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"BiLSTM 原始测试集准确率 (基线): {bilstm_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPIU6w95-Dc1",
        "outputId": "7e8712f0-b720-43a8-d734-87c751215ea3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 训练 BiLSTM ---\n",
            "Epoch 1/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 17ms/step - accuracy: 0.8735 - loss: 0.2257 - val_accuracy: 0.9912 - val_loss: 0.0669\n",
            "Epoch 2/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9738 - loss: 0.0963 - val_accuracy: 0.9751 - val_loss: 0.0857\n",
            "Epoch 3/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9608 - loss: 0.1235 - val_accuracy: 0.7221 - val_loss: 0.4007\n",
            "Epoch 4/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9797 - loss: 0.0594 - val_accuracy: 0.9978 - val_loss: 0.0077\n",
            "Epoch 5/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.9990 - loss: 0.0050 - val_accuracy: 0.9985 - val_loss: 0.0025\n",
            "Epoch 6/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9959 - loss: 0.0124 - val_accuracy: 0.9993 - val_loss: 0.0035\n",
            "Epoch 7/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9978 - val_loss: 0.0093\n",
            "Epoch 8/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9985 - loss: 0.0077 - val_accuracy: 0.9993 - val_loss: 0.0011\n",
            "Epoch 9/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 1.0000 - loss: 3.4414e-04 - val_accuracy: 1.0000 - val_loss: 4.0350e-04\n",
            "Epoch 10/10\n",
            "\u001b[1m192/192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 1.4990e-04 - val_accuracy: 1.0000 - val_loss: 3.2630e-04\n",
            "BiLSTM 原始测试集准确率 (基线): 0.9965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2.2 TFBert 模型 ---\n",
        "train_encodings = encode_data(X_train_raw, bert_tokenizer)\n",
        "test_encodings = encode_data(X_test_raw, bert_tokenizer)\n",
        "\n",
        "def build_bert_model(model_name):\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=1,\n",
        "        from_pt=True\n",
        "    )\n",
        "\n",
        "    # 关键修复 1: 使用字符串 'adam' 绕过对象识别错误\n",
        "    optimizer_name = 'adam'\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    model.compile(optimizer=optimizer_name, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # 关键修复 2: 编译后，手动设置正确的学习率 (3e-5)\n",
        "    model.optimizer.learning_rate.assign(3e-5)\n",
        "\n",
        "    return model\n",
        "\n",
        "bert_model = build_bert_model(BERT_MODEL_NAME)\n",
        "\n",
        "print(\"\\n--- 训练 TFBert ---\")\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train)).shuffle(100).batch(16)\n",
        "\n",
        "bert_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=3,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test)).batch(16)\n",
        "_, bert_acc = bert_model.evaluate(test_dataset, verbose=0)\n",
        "print(f\"TFBert 原始测试集准确率 (基线): {bert_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCTgz7ZS_fwi",
        "outputId": "5040e2ae-2617-4aa4-b3fc-e9a49974e177"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 训练 TFBert ---\n",
            "Epoch 1/3\n",
            "853/853 [==============================] - 264s 261ms/step - loss: 0.0334 - accuracy: 0.9895\n",
            "Epoch 2/3\n",
            "853/853 [==============================] - 225s 264ms/step - loss: 0.0077 - accuracy: 0.9977\n",
            "Epoch 3/3\n",
            "853/853 [==============================] - 225s 263ms/step - loss: 0.0069 - accuracy: 0.9984\n",
            "TFBert 原始测试集准确率 (基线): 0.9984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3.1 核心替换函数（中文同义词代理，作为 PSO 的搜索空间） ---\n",
        "def get_chinese_synonym_candidates_proxy(word):\n",
        "    # 保持与之前的 Sememe 替换的词汇粒度一致\n",
        "    substitution_map = {\n",
        "        '银行': ['金融机构', '分行', '机构'],\n",
        "        '验证码': ['校验码', '代码', '数字'],\n",
        "        '转账': ['汇款', '划款', '打钱'],\n",
        "        '密码': ['口令', '密钥'],\n",
        "        '账户': ['卡号', '名下'],\n",
        "        '贷款': ['借款', '借钱'],\n",
        "        '您好': ['你好', '喂']\n",
        "    }\n",
        "\n",
        "    if len(word) <= 1 or word in ['的', '是', '了', '吗', '请问', '和', '也']:\n",
        "        return []\n",
        "\n",
        "    # 返回原始词和所有同义词，形成一个搜索空间\n",
        "    candidates = substitution_map.get(word, [])\n",
        "    if candidates:\n",
        "        return [word] + candidates\n",
        "    return []\n",
        "\n",
        "# --- 3.2 对抗攻击：简化的离散 PSO (Particle Swarm Optimization) ---\n",
        "\n",
        "# PSO 参数\n",
        "POPULATION_SIZE = 10  # 粒子数量\n",
        "MAX_ITER = 5          # 最大迭代次数\n",
        "C1 = 0.5              # 认知项权重 (pBest)\n",
        "C2 = 0.5              # 社会项权重 (gBest)\n",
        "W = 0.8               # 惯性权重\n",
        "\n",
        "def get_word_to_index_map(words, candidates_fn):\n",
        "    \"\"\"创建词语在候选空间中的索引映射\"\"\"\n",
        "    word_to_index = {}\n",
        "    index_to_word = {}\n",
        "\n",
        "    for i, word in enumerate(words):\n",
        "        candidates = candidates_fn(word)\n",
        "        if candidates:\n",
        "            word_to_index[i] = {cand: idx for idx, cand in enumerate(candidates)}\n",
        "            index_to_word[i] = candidates\n",
        "    return word_to_index, index_to_word\n",
        "\n",
        "# 修复后的 get_text_from_position 函数需要依赖 perform_attack_pso 内部的局部变量\n",
        "def get_prediction_score(text, target_model, tokenizer_fn):\n",
        "    \"\"\"计算模型对文本的预测分数和标签 (兼容 Keras 和 TFBert)\"\"\"\n",
        "    model_input = tokenizer_fn([text])\n",
        "    raw_output = target_model.predict(model_input, verbose=0)\n",
        "\n",
        "    # 1. 处理模型输出：统一为 logits/probability tensor\n",
        "    if isinstance(raw_output, tuple) and hasattr(raw_output, 'logits'):\n",
        "        # Hugging Face Model (TFBert): Output is a tuple with logits\n",
        "        output_tensor = raw_output.logits\n",
        "        is_hf_bert = True\n",
        "    else:\n",
        "        # Keras Functional Model (BiLSTM): Output is the probability tensor\n",
        "        # TFBert also falls here if predict returns raw tensor\n",
        "        output_tensor = raw_output\n",
        "        # 我们只能通过检查模型类型来确定它是否是 BiLSTM\n",
        "        is_hf_bert = hasattr(target_model, 'config') # 简单检查是否存在 config 属性来判断是否为 HF 模型\n",
        "\n",
        "    # 2. 确定概率值 (P(Fraud=1))\n",
        "    # 关键：获取模型名称来判断是否需要 sigmoid\n",
        "    if is_hf_bert:\n",
        "        # TFBert with num_labels=1 始终输出原始 logits，需要应用 sigmoid\n",
        "        # 注意：这里假设 TFBert model.predict() 成功返回了 logits\n",
        "        try:\n",
        "            prob_original_class = tf.nn.sigmoid(output_tensor[0][0]).numpy()\n",
        "        except:\n",
        "            # 如果 output_tensor 已经是 numpy array，直接取值\n",
        "            prob_original_class = tf.nn.sigmoid(output_tensor[0][0]).numpy() if tf.is_tensor(output_tensor) else output_tensor[0][0]\n",
        "    else:\n",
        "        # BiLSTM Keras model: 输出层是 sigmoid，所以 output_tensor 已经是概率\n",
        "        prob_original_class = output_tensor[0][0]\n",
        "\n",
        "    # 3. 计算攻击分数和标签\n",
        "    # 攻击分数 (score): 目标是欺骗模型，即最大化错误的类别 0 (非欺诈) 的概率。\n",
        "    # score = 1.0 - P(Fraud=1)\n",
        "    score = 1.0 - prob_original_class\n",
        "    original_label = 1 if prob_original_class > 0.5 else 0\n",
        "\n",
        "    return original_label, score\n",
        "\n",
        "def perform_attack_pso(original_text, target_model, tokenizer_fn, get_candidates_fn, max_modifications=3):\n",
        "    \"\"\"\n",
        "    使用简化的离散 PSO 算法执行攻击\n",
        "    \"\"\"\n",
        "    # 内部定义 get_text_from_position 需要访问局部变量\n",
        "    global words, modifiable_indices, index_to_word\n",
        "\n",
        "    words = list(jieba.cut(original_text))\n",
        "    original_label, _ = get_prediction_score(original_text, target_model, tokenizer_fn)\n",
        "\n",
        "    # 1. 初始化粒子和搜索空间\n",
        "    word_to_index, index_to_word = get_word_to_index_map(words, get_candidates_fn)\n",
        "    modifiable_indices = list(word_to_index.keys())\n",
        "\n",
        "    if not modifiable_indices:\n",
        "        return False, original_text, original_label, original_label, 0\n",
        "\n",
        "    num_dims = len(modifiable_indices)\n",
        "\n",
        "    # 在内部定义 get_text_from_position 以访问局部变量 words, modifiable_indices, index_to_word\n",
        "    def get_text_from_position(position_indices):\n",
        "        temp_words = words[:]\n",
        "        for dim, word_idx in enumerate(modifiable_indices):\n",
        "            cand_list = index_to_word[word_idx]\n",
        "            cand_index = position_indices[dim]\n",
        "            # 关键修复：确保索引是整数\n",
        "            temp_words[word_idx] = cand_list[int(cand_index)]\n",
        "        return \"\".join(temp_words)\n",
        "\n",
        "    # 初始化粒子的 \"位置\" (X) 和 \"速度\" (V)\n",
        "    particle_positions = []  # 粒子位置 (词语索引)\n",
        "    particle_velocities = [] # 粒子速度 (连续值)\n",
        "\n",
        "    for _ in range(POPULATION_SIZE):\n",
        "        pos = []\n",
        "        vel = []\n",
        "        for word_idx in modifiable_indices:\n",
        "            # 随机选择一个初始位置 (词语索引)\n",
        "            cand_len = len(index_to_word[word_idx])\n",
        "            pos.append(np.random.randint(0, cand_len))\n",
        "            # 随机初始化速度\n",
        "            vel.append(np.random.rand() * 2 - 1)\n",
        "        particle_positions.append(np.array(pos, dtype=float)) # 保持 float 类型以便后续速度相加\n",
        "        particle_velocities.append(np.array(vel, dtype=float))\n",
        "\n",
        "    # 初始化 pBest (个体最优) 和 gBest (全局最优)\n",
        "    pbest_positions = [pos.copy() for pos in particle_positions]\n",
        "    pbest_scores = np.full(POPULATION_SIZE, -1e9)\n",
        "\n",
        "    gbest_position_idx = 0\n",
        "    gbest_score = -1e9\n",
        "\n",
        "    # 2. PSO 迭代\n",
        "    for iter_i in range(MAX_ITER):\n",
        "        # 评估所有粒子\n",
        "        for i in range(POPULATION_SIZE):\n",
        "            current_pos = particle_positions[i]\n",
        "            # 关键修复：初始化 current_text 为原始文本，确保它始终有值\n",
        "            current_text = original_text\n",
        "\n",
        "            # 检查修改次数是否超过限制\n",
        "            modified_count = 0\n",
        "            for dim, word_idx in enumerate(modifiable_indices):\n",
        "                # 假设原始词总是候选列表的第 0 个 (或最相似的那个)\n",
        "                if int(current_pos[dim]) != 0:\n",
        "                    modified_count += 1\n",
        "\n",
        "            if modified_count > max_modifications:\n",
        "                current_score = -1e9 # 惩罚超过修改限制的粒子\n",
        "            else:\n",
        "                current_text = get_text_from_position(current_pos)\n",
        "                _, current_score = get_prediction_score(current_text, target_model, tokenizer_fn)\n",
        "\n",
        "            # 更新 pBest\n",
        "            if current_score > pbest_scores[i]:\n",
        "                # 修复 DeprecationWarning：确保 current_score 是标量\n",
        "                pbest_scores[i] = current_score.item() if isinstance(current_score, np.ndarray) else current_score\n",
        "                pbest_positions[i] = current_pos.copy()\n",
        "\n",
        "            # 更新 gBest\n",
        "            if current_score > gbest_score:\n",
        "                gbest_score = current_score.item() if isinstance(current_score, np.ndarray) else current_score\n",
        "                gbest_position_idx = i\n",
        "\n",
        "            # 检查是否攻击成功\n",
        "            # 只有在未被惩罚时才检查攻击成功\n",
        "            if modified_count <= max_modifications:\n",
        "                adv_label, _ = get_prediction_score(current_text, target_model, tokenizer_fn)\n",
        "                if adv_label != original_label:\n",
        "                    # 攻击成功，返回当前对抗样本\n",
        "                    original_words = list(jieba.cut(original_text))\n",
        "                    modified_words = list(jieba.cut(current_text))\n",
        "                    num_modified = sum(1 for w1, w2 in zip(original_words, modified_words) if w1 != w2)\n",
        "                    return True, current_text, original_label, adv_label, num_modified\n",
        "\n",
        "        # 更新粒子速度和位置\n",
        "        gbest_pos = pbest_positions[gbest_position_idx]\n",
        "\n",
        "        for i in range(POPULATION_SIZE):\n",
        "            r1 = np.random.rand(num_dims)\n",
        "            r2 = np.random.rand(num_dims)\n",
        "\n",
        "            # 速度更新公式: V_new = W * V + C1 * R1 * (Pbest - X) + C2 * R2 * (Gbest - X)\n",
        "            cognitive_component = C1 * r1 * (pbest_positions[i] - particle_positions[i])\n",
        "            social_component = C2 * r2 * (gbest_pos - particle_positions[i])\n",
        "\n",
        "            # 更新速度\n",
        "            new_velocity = W * particle_velocities[i] + cognitive_component + social_component\n",
        "            particle_velocities[i] = new_velocity\n",
        "\n",
        "            # 位置更新: X_new = X + V_new (离散化)\n",
        "            new_position = particle_positions[i] + particle_velocities[i]\n",
        "\n",
        "            # 确保位置 (词语索引) 有效\n",
        "            for dim, word_idx in enumerate(modifiable_indices):\n",
        "                cand_len = len(index_to_word[word_idx])\n",
        "                # 裁剪位置到 [0, cand_len-1] 范围内并四舍五入\n",
        "                new_position[dim] = np.clip(np.round(new_position[dim]), 0, cand_len - 1)\n",
        "\n",
        "            particle_positions[i] = new_position\n",
        "\n",
        "\n",
        "    # 3. 迭代结束，返回全局最优解 (可能未成功攻击)\n",
        "    final_pos = pbest_positions[gbest_position_idx]\n",
        "    final_adv_text = get_text_from_position(final_pos)\n",
        "    final_adv_label, _ = get_prediction_score(final_adv_text, target_model, tokenizer_fn)\n",
        "\n",
        "    is_successful = (original_label != final_adv_label)\n",
        "\n",
        "    original_words = list(jieba.cut(original_text))\n",
        "    modified_words = list(jieba.cut(final_adv_text))\n",
        "    num_modified = sum(1 for w1, w2 in zip(original_words, modified_words) if w1 != w2)\n",
        "\n",
        "    return is_successful, final_adv_text, original_label, final_adv_label, num_modified\n",
        "\n",
        "# --- 3.3 封装模型输入函数 (不变) ---\n",
        "def bilstm_tokenizer_fn(texts):\n",
        "    seg_texts = chinese_segment(texts)\n",
        "    seq = tokenizer.texts_to_sequences(seg_texts)\n",
        "    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "def bert_tokenizer_fn(texts):\n",
        "    return encode_data(texts, bert_tokenizer)\n",
        "\n",
        "# --- 4.1 实验一: Synonym+PSO 攻击 BiLSTM/TFBert ---\n",
        "\n",
        "def run_attack_experiment_pso(model, tokenizer_fn, model_name, X_test, y_test, get_candidates_fn):\n",
        "\n",
        "    X_test_enc = tokenizer_fn(X_test)\n",
        "    raw_output = model.predict(X_test_enc, verbose=0)\n",
        "\n",
        "    # 修复：明确根据模型名称处理输出\n",
        "    if model_name == \"TFBert\":\n",
        "        # 提取 logits\n",
        "        if hasattr(raw_output, 'logits'):\n",
        "            logits = raw_output.logits\n",
        "        else:\n",
        "            logits = raw_output # 假设是原始张量\n",
        "\n",
        "        if logits.shape[-1] == 1:\n",
        "             predictions = (tf.nn.sigmoid(logits).numpy() > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "             predictions = np.argmax(logits, axis=1)\n",
        "    else: # BiLSTM\n",
        "        predictions = (raw_output > 0.5).astype(int).flatten()\n",
        "\n",
        "    correctly_classified_indices = np.where(predictions == y_test)[0]\n",
        "\n",
        "    X_attack = X_test.iloc[correctly_classified_indices]\n",
        "    y_attack = y_test[correctly_classified_indices]\n",
        "\n",
        "    print(f\"\\n--- {model_name} 攻击 (PSO) 开始 ---\")\n",
        "    print(f\"原始正确分类样本数 (攻击目标): {len(X_attack)}\")\n",
        "\n",
        "    success_count = 0\n",
        "    modified_texts = []\n",
        "    modification_rates = []\n",
        "\n",
        "    # 由于 PSO 攻击耗时较长，我们只对前 50 个样本进行攻击以加快运行速度\n",
        "    X_sample = X_attack.head(50)\n",
        "    y_sample = y_attack[:50]\n",
        "\n",
        "    for idx, (original_text, original_label) in enumerate(zip(X_sample, y_sample)):\n",
        "        is_successful, adv_text, _, adv_label, num_modified = perform_attack_pso(\n",
        "            original_text,\n",
        "            model,\n",
        "            tokenizer_fn,\n",
        "            get_candidates_fn,\n",
        "            max_modifications=3\n",
        "        )\n",
        "\n",
        "        modified_texts.append(adv_text)\n",
        "\n",
        "        original_len = len(list(jieba.cut(original_text)))\n",
        "        modification_rates.append(num_modified / original_len if original_len > 0 else 0)\n",
        "\n",
        "        if is_successful:\n",
        "            success_count += 1\n",
        "            if success_count <= 2:\n",
        "                print(f\"\\n[成功案例] 原文: {original_text}\")\n",
        "                print(f\"[成功案例] 对抗样本: {adv_text} (修改词数: {num_modified})\")\n",
        "\n",
        "    asr = success_count / len(X_sample) if len(X_sample) > 0 else 0\n",
        "    avg_mod_rate = np.mean(modification_rates) if modification_rates else 0\n",
        "    print(f\"\\n模型: {model_name}\")\n",
        "    print(f\"攻击成功率 (ASR): {asr:.4f} ({success_count}/{len(X_sample)})\")\n",
        "    print(f\"平均改动率: {avg_mod_rate:.4f}\")\n",
        "\n",
        "    # 为了评估，我们将攻击成功的样本和未成功的样本合并回原始的攻击目标集合中\n",
        "    adv_test_set = pd.DataFrame({'text': modified_texts, 'label': y_sample})\n",
        "    return asr, avg_mod_rate, adv_test_set\n",
        "\n",
        "# 运行 BiLSTM 攻击\n",
        "bilstm_asr, bilstm_mod_rate, adv_bilstm_df = run_attack_experiment_pso(\n",
        "    bilstm_model, bilstm_tokenizer_fn, \"BiLSTM\", X_test_raw, y_test, get_chinese_synonym_candidates_proxy\n",
        ")\n",
        "\n",
        "# 运行 TFBert 攻击\n",
        "bert_asr, bert_mod_rate, adv_bert_df = run_attack_experiment_pso(\n",
        "    bert_model, bert_tokenizer_fn, \"TFBert\", X_test_raw, y_test, get_chinese_synonym_candidates_proxy\n",
        ")\n",
        "\n",
        "# --- 4.2 结果分析 (准确率下降情况) ---\n",
        "def evaluate_on_adv_data(model, tokenizer_fn, adv_df, model_name, original_acc):\n",
        "    if adv_df.empty:\n",
        "        print(f\"模型 {model_name} 在改写数据集 (D_adv) 上的准确率：0 (无成功攻击样本)\")\n",
        "        return\n",
        "\n",
        "    X_adv_enc = tokenizer_fn(adv_df['text'])\n",
        "    y_true = np.array(adv_df['label'])\n",
        "\n",
        "    # 兼容 Keras 和 TFBert 的预测结果\n",
        "    raw_output = model.predict(X_adv_enc, verbose=0)\n",
        "\n",
        "    # 修复：明确根据模型名称处理输出\n",
        "    if model_name == \"TFBert\":\n",
        "        # 提取 logits\n",
        "        if hasattr(raw_output, 'logits'):\n",
        "            logits = raw_output.logits\n",
        "        else:\n",
        "            logits = raw_output # 假设是原始张量\n",
        "\n",
        "        if logits.shape[-1] == 1:\n",
        "            # TFBert 是 logits，需要 sigmoid 才能判断\n",
        "            preds = (tf.nn.sigmoid(logits).numpy() > 0.5).astype(int).flatten()\n",
        "        else:\n",
        "            preds = np.argmax(logits, axis=1)\n",
        "    else:\n",
        "        # Keras BiLSTM (输出已经是概率)\n",
        "        preds = (raw_output > 0.5).astype(int).flatten()\n",
        "\n",
        "    adv_acc = accuracy_score(y_true, preds)\n",
        "    print(f\"\\n模型 {model_name} 在改写数据集 (D_adv) 上的准确率: {adv_acc:.4f}\")\n",
        "\n",
        "    # 注意：这里的原始准确率 (original_acc) 是针对完整的测试集 X_test_raw 计算的，\n",
        "    # 而 adv_df 只包含 X_sample 的结果。因此，这个下降比例仅供参考。\n",
        "    print(f\"（原始准确率 {original_acc:.4f} 下降到 {adv_acc:.4f}，基于前 {len(y_true)} 个攻击目标）\")\n",
        "\n",
        "evaluate_on_adv_data(bilstm_model, bilstm_tokenizer_fn, adv_bilstm_df, \"BiLSTM\", bilstm_acc)\n",
        "evaluate_on_adv_data(bert_model, bert_tokenizer_fn, adv_bert_df, \"TFBert\", bert_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xcXLmZiGXlt",
        "outputId": "e0bdb708-5bdb-4c8e-a0a8-bff6b5782bf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- BiLSTM 攻击 (PSO) 开始 ---\n",
            "原始正确分类样本数 (攻击目标): 2539\n",
            "\n",
            "模型: BiLSTM\n",
            "攻击成功率 (ASR): 0.0000 (0/50)\n",
            "平均改动率: 0.0504\n",
            "\n",
            "--- TFBert 攻击 (PSO) 开始 ---\n",
            "原始正确分类样本数 (攻击目标): 2543\n",
            "\n",
            "模型: TFBert\n",
            "攻击成功率 (ASR): 0.0000 (0/50)\n",
            "平均改动率: 0.0040\n",
            "\n",
            "模型 BiLSTM 在改写数据集 (D_adv) 上的准确率: 1.0000\n",
            "（原始准确率 0.9965 下降到 1.0000，基于前 50 个攻击目标）\n",
            "\n",
            "模型 TFBert 在改写数据集 (D_adv) 上的准确率: 1.0000\n",
            "（原始准确率 0.9984 下降到 1.0000，基于前 50 个攻击目标）\n"
          ]
        }
      ]
    }
  ]
}